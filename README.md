# MODULE 1


# DATA ANALYTICS
# Chapter 1

# What will be covered 
* understanding what role of Data Analyst is and the scope of Data Analytics
* Compare and contrast differenet data types
* Compare and contast data structrures and file fomarts
* Learner assuspmtions
* The process of data analystics
* The techniques of data analyst
* The role o data governance

# WHAT IS DATA ANALYITICS

data analytics is the whole process of taking raw data and turning it into insights that can be used to make decisions. This involves collecting the data, cleaning it, analyzing it, and visualizing the results.

# several key responsibilities and skills of data analyitics are:

# Data Collection and Preparation:

# Collection:
Involves gathering the data you need for your analysis. This can come from various sources, like surveys, databases, external sources ,social media, or even physical experiments. Here, you decide what information is relevant to answer your question and go get it.

# Preparation:
Is all about getting that raw data(Unprocessed, Unstructured, Unrefined) into a usable shape. This often involves cleaning the data by fixing errors, filling in missing bits, and ensuring consistency. You might also need to transform the data, like converting dates into a standard format, or combining data from different sources into a single, unified format.

# Data Analysis:
Employing statistical methods, machine learning techniques, or other analytic tools to interpret data,
Identifying trends, patterns, and correlations that might not be immediately obvious.

# Data Visualization and Storytelling:

Creating visual representations of the data, such as charts, graphs, and dashboards, to make complex information easily understandable,
Articulating findings in a compelling narrative to communicate the significance of the data to stakeholders.

# Decision Support:
Making recommendations based on data-driven insights to help guide business decisions,
Providing context around the data, including potential implications and future trends.

# Collaboration and Communication:

Working closely with other departments, such as marketing, finance, and operations, to understand their data needs and provide insights,
Effectively communicating complex data findings in a clear and concise manner to non-technical stakeholders.

# Continuous Learning and Adaptation:

Keeping up-to-date with the latest industry trends, tools, and technologies in data analysis.
Adapting to new types of data and analytical methods as the organization's needs evolve.

In conclusin By fulfilling these roles, a data analyst helps an organization make informed decisions that can improve operational efficiencies, drive business strategies, and create a competitive advantage. The goal is to contribute to the organization's success by turning data into a valuable asset that informs and drives decision-making.


# What makes up data analytics 

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/8c119eb6-2547-4fa9-9482-f078ace610a3)

# The Analytics Process

Analysts working with data move through a series of different steps as they seek to gain business value from their organization's data.

# illustration of the process that is used by Analysts as they aquire new data, clean and manipulate that data, analyz it, create visualizations, and then reports and commmunicate thier results to business leaders

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/df0ec801-f364-4573-bdd7-15803f2c126a)


# The Analytics Process is Iterative

analyzing data is actually a looping process, not a straight line.  You might go through the steps like defining a question, collecting data, cleaning it, analyzing it, and visualizing it, but you might also need to go back and redo a step if you find something unexpected. However The whole thing is meant to be a flexible guide, not a set of strict rules.

# Analytics Techniques

Data analysts use a bunch of different tools and tricks to make sense of their data.  These techniques can be grouped together based on what the analyst is trying to do, or what kind of tools they're using. The passage is basically saying we'll be diving into these different categories of techniques next.

e.g 
* Descriptive Analytics
* predictive Analytics
* Perspective Analytics

# Machine Learning, Artificial Intelligence, and Deep Learning

Artificial intelligence (AI) is the broadest term, encompassing any technique that makes a computer mimic human behavior. Machine learning (ML) is a subset of AI that uses statistical methods to find patterns in data, allowing the computer to learn without explicit programming. Deep learning is an even more specialized subset of machine learning that uses complex neural networks to analyze data, particularly useful for images, videos, and sound.

# illustration that shows the relationship between the above fields
![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/1a624b4b-07d5-4410-ae68-a25f5a0f1260)

# Data Governance 

Data governance programs ensure that the organization has high-quality data and is able to effectively control that data.

# Analytics Tools

Software helps analysts work through each one of the phases of the analytics process. These tools automate much of the heavy lifting of data analysis, improving the analyst's ability to acquire, clean, manipulate, visualize, and analyze data. They also provide invaluable assistance in reporting and communicating results.

# SUMMARY
Businesses are sitting on a goldmine of data! Analytics programs help them unlock this value by turning data into insights.  These programs are becoming more accessible due to:

A massive amount of data being available
Cheaper data storage
Powerful cloud computing
This creates a big opportunity for businesses and especially for data professionals who can analyze this data effectively.

Here's the typical process an analyst follows:

Gather data from various sources (internal and external).
Clean and organize the data.
Analyze the data to draw conclusions.
Create visualizations (charts, graphs) to present the findings clearly.
Develop reports and dashboards to communicate the insights to business leaders.

# CHAPTER 2 

# Understanding Data 

This chapter will focus on Understand Domain 1.0:
* Data Concepts and Environments
* Compare and contrast different data types
* Compare and contrast common data structures and file formats

for one to understand data types one needs to understand data elements.

# DATA ELEMENTS
A data element is an attribute about a person, place, or thing containing data within a range of values. Data elements also describe characteristics of activities, including orders, transactions, and events. 

example of data elements 
![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/06376ecc-cb78-4f1a-8077-b018eecd22a0)

# Tabular Data

Tabular data is data organized into a table, made up of columns and rows. A table represents information about a single topic. Each column represents a uniquely named field within a table, also called a variable, about a single characteristic. The contents of each column contain values for the data element as defined by the column header.

# Structured Data Types
Structured data is like a well-organized spreadsheet. Imagine a table with rows and columns, where each column has a clear heading defining the type of data it contains (like names, dates, or prices). This makes structured data easy for computers to understand and work with, just like it's easy for us to read and analyze information in a spreadsheet.

# most common data types that give structured data its structure.

# character 

This data type acts like a filter for data entry, ensuring only valid characters are entered. These characters can be letters, numbers, or symbols from your keyboard. There are different character data types available, each with its own limit on the number of characters allowed.

# Alphanumeric

Alphanumeric data type is your go-to for storing text that combines letters and numbers. It's the most common type for character data. Imagine product codes (SKUs) in retail stores. These codes often mix letters representing brands or styles with numbers indicating sizes or variations. The alphanumeric data type ensures you can store these SKUs effectively, allowing you to track inventory across different systems.

Example 
![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/0556a37f-4773-4874-b1ce-f309c0022cd8)
# NEED TO GO OVER FROM HERE DONT UNDERSTAND AS YET



# Unstructured Data Types
Unstructured data types refer to information that doesn't have a predefined format or organization. Unlike the neat rows and columns of structured data (like spreadsheets), unstructured data is more like a messy attic filled with all sorts of things.

Examples of unstructured data include digital images, audio recordings, video recordings, and open-ended survey responses. Analyzing unstructured data creates a wealth of information and insight. Many people have camera-enabled smartphones, and using video for conversations and meetings is commonplace. To capture and analyze unstructured data, we make use of data types designed explicitly for that purpose

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/fb1711f0-cad1-4e17-bdc8-04449340f0e3)


# Categories of Data

The world of data isn't as clear-cut as structured vs unstructured. Semi-structured data bridges the gap between those two categories.

Imagine a spectrum: on one end, you have highly organized data in tables (structured). On the other end, you have completely free-form information like videos (unstructured). Semi-structured data sits in the middle. It has some organization, but not a rigid structure like a spreadsheet. Emails or social media posts are good examples. They might have elements like sender, recipient, or timestamps, but the content itself is more flexible. This makes semi-structured data easier to work with than completely unstructured data, but it still requires some wrangling before analysis.

explanation of the distinction between quantitative and qualitative data!

* Quantitative data: Expressed in numbers and represents measurable qualities. It answers questions like "how many" or "how much." (e.g., height, weight, temperature)
* Qualitative data: Descriptive information, often in the form of text. It describes characteristics or qualities and answers questions like "why" or "what." (e.g., pet name, color, customer opinion)

  explanation of discrete and continuous data

# Discrete data:
is like counting whole things. You can't have parts of things, so it applies to things you can count (e.g., number of pets, number of chickens in whole or half portions).
  
# Continuous data
 is for measurements that can have any value within a range, often using decimals (e.g., height, weight).
Even though age is continuous (technically has infinite values), we often treat it as discrete by using whole years.

This distinction between discrete and continuous data is important for understanding how we analyze and interpret numbers.


Dimensional modeling helps us organize data for analysis! Here's the gist:

It separates data into two main tables:
# Fact tables:
Store the quantitative measurements we care about (e.g., appointment data for a veterinary clinic).
# Dimension tables: 
Provide context for the facts, describing "who, what, when, where, why" (e.g., details on veterinarians, owners, procedures, and pets in the appointment example).

Imagine a fact table like a big basket holding the key data points, and the dimension tables act like labels stuck on the basket, giving details about the contents. This structure makes it easier to analyze the data and answer questions.


  ![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/f295e27d-0b7d-42d3-84d7-dcfa679a5ad4)


# Dimenstional data:
* it is aall about organizing details arouna a central theme.
* It grou[s individual details(attributes) about a specific subject
* Each group, like the "pets" table with pet info, is called a dimension

  Imagine a filing cabinet for a vet clinic. Each drawer (dimension) holds details about a specific aspect, like pets (their breed, age, etc.) or owners (their contact information). When you combine information from these drawers (dimensions) with the main patient file (fact table), you get a complete picture for each appointment.
  
# Common Data Structures

* Structured data needs a neat and tidy filing system (think database schemas) to be useful.
* Unstructured data, like emails and videos, is more flexible in how it's stored.
Analysts juggle multiple tools, so smooth data exchange (interoperability) is crucial.
To boost efficiency, standardized practices have emerged for both structured and unstructured data organization.

Tabular data is structured data, with values stored in a consistent, defined manner, organized into columns and rows. Data is consistent when all entries in a column contain the same type of value. This method of organization facilitates aggregation

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/2666c426-0165-448d-b2b1-ca28edbc0662)

# Unstructured Data

* What it is: Descriptive information like images, text, audio/video (qualitative).
  
* Key characteristics: Highly variable and lacks a predefined structure.
  
* Storage: Different from structured data due to its variety.
  
* Importance: Huge potential for businesses to extract value (over 90% according to Forbes).
  
* Example: Machine data (digital footprints from devices) is a goldmine of unstructured data.
  
* Storage Technologies: Utilize unique identifiers to link data elements.

Unstructured data might seem messy, but it holds immense value if we can unlock it!

* Object storage facilitates the storage of unstructured data. The key-value concept underpins the design of object storage. The key is a unique identifier, and the value is the unstructured data itself.

the key is the filename, and the value is the contents of the file itself. Note that in the bellow figure, each file is of a different type. The word document.docx is a Microsoft Word file, textfile.txt contains plain-text data, and the png image.png and lp_image-8.jpeg objects are digital images.

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/302f40d1-6b77-44ad-a68d-8a94a2475135)


To access the contents of a file, you need to know its key

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/c6a019fc-e53b-4bf5-ab58-ac5f2fb7688e)

# Semi-Structured Data

* Think of it as: Structured, but not rigid like a table (tabular).
Example: Emails have a sender, recipient, subject, etc. (structured), but the body and attachments can be anything (unstructured).

* Benefits: Easier to work with than completely unstructured data.
  
* Key feature: Uses tags or separators to add context to the data.
Semi-structured data provides some organization without being as strict as a traditional database.

# Common File Formats

* Different file formats exist to store and exchange data efficiently between various tools and applications.
  
* Some formats have become widely accepted standards, and data analysts should be familiar with these for various tasks.

# Text Files

* Super common data format for computers.
* Plain text only (letters, numbers, symbols).
* No fancy formatting or images.
* Opens easily on pretty much any device (PC, Mac, Linux).
* Also called "flat files".

A unique character known as a delimiter 

* Delimiters: Special characters used to separate data points in text files (like commas or tabs).
  
* Why? Makes structured data transmission easier within plain text files.
  
* Common Delimiters: Comma (CSV) and tab (TSV) are widely accepted standards.
  
* Benefits: Many software programs and coding languages can easily read and write these delimited text files.
  
Exporting as CSV or TSV



![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/7f679b11-91c8-4daa-9e2f-fb61faf02f15)

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/2d667fa4-f0c8-41c6-896a-1ed8945b251d)

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/1b449707-19cb-4038-92d1-5bf522d84d59)

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/015c6617-198c-4f2f-9411-dbd56d1fe3c3)


# JavaScript Object Notation

* What it is: A popular file format for storing data in a human-readable and machine-friendly way.
  
Key features:
* Open standard and lightweight.
* Easy for both humans and computers to understand and process.
* Supported by many programming languages (Python, R, Go, etc.).
  
* Uses: Often used to exchange data between applications or store configuration settings.

# Extensible Markup Language (XML)


Extensible Markup Language (XML) is a markup language that facilitates structuring data in a text file. While conceptually similar to JSON, XML incurs more overhead because it makes extensive use of tags. Tags describe a data element and enclose each value for each data element. While these tags help readability, they add a significant amount of overhead.

# HyperText Markup Language (HTML)
HyperText Markup Language (HTML) is a markup language for documents designed to be displayed in a web browser. HTML pages serve as the foundation for how people interact with the World Wide Web. Similar to XML, HTML is a tag-based language


Most people interact with HTML as interpreted by a web browser. HTML has become increasingly sophisticated over the years, with the ability for developers to create web pages that dynamically display content, adjust to different screen sizes, and play videos. Among the many tags that HTML supports is the image tag. It would be possible to display a picture of each pet on the table using image tags



SUMMARY 
Here's a summary of the key points about data types and formats:

Choosing Data Types:

* The type of data (numbers, text, dates, etc.) you're working with influences how you store and analyze it.
* Choosing the right data type (discrete, continuous, categorical) improves data quality.
  
# Structured Data:

* Think of data organized in tables with rows and columns.
* Common formats for structured data include:
  
CSV (Comma-Separated Values): Simple text files for data exchange.
JSON (JavaScript Object Notation): Flexible and human-readable format for structured data.
XML (Extensible Markup Language): Powerful format for complex data structures and metadata.

# Unstructured Data:

* Data like images, audio, and video doesn't have a predefined structure.
Web Data:

* HTML (Hypertext Markup Language) is the standard for structuring web pages, allowing programmatic interaction with data.
Key Takeaway:

Understanding data types and formats is essential for data analysts to effectively work with and analyze information.

# 1. Consider the values of what you will store before selecting data types.â€ƒ

# Numbers:
* Decimals: Use a numeric data type.
* Whole numbers: Use an integer data type.
* Avoid currency-specific types: They can cause calculation issues.
# Text:
* Use the alphanumeric data type.
# Dates:
* Choose a data type that includes time if needed.
# Non-textual data (audio, video, images): Use a BLOB (Binary Large Object) data type.
By choosing the right data type, you ensure your data is stored efficiently and avoids errors during calculations or analysis.

# . Know that you can format data after storing it.

2. Know that you can format data after storing it.

# Data types:

* Decide how data is stored (e.g., numbers with decimals, whole numbers only).
# Data formatting:

* Controls how data is presented for people (e.g., show all decimals, round to two, display as currency).
Think of it like this:

# Data type:
* is like filing a document in a specific folder (e.g., "numbers" folder).

# Formatting: 
* is like decorating the folder label (e.g., show two decimal places on the label).

# 3. Consider the absolute limits of values that you will use before selecting data types.
When selecting data types, consider the range of values that a data element can contain. Suppose the values need to fall within a given, defined range. In that case, you must select a data element that can support discrete data. If the data element's range is unknown, a data element that supports continuous data is necessary.

4. Explain the differences between structured and unstructured data

# Structured data: 
Highly organized information like a table.

* Think "rectangular": Rows and columns with consistent data types in each column.
* Easier to analyze using traditional techniques.

# Unstructured data:
Lacks a predefined structure, like text documents or images.

* Doesn't fit neatly into columns and rows.
* Requires more advanced analysis techniques to find patterns.
* 
Imagine data as Legos. Structured data is like pre-built sets with clear instructions. Unstructured data is like a box of random Legos - it takes more effort to build something meaningful.

# 5. Understand the differences in common file formats

# Structured data, easy exchange:
These formats make data readable and transferable between different tools.

# Delimiters: 
Special characters (like commas) separate data points in text files.
# CSV (Comma-Separated Values):
A popular format using commas as delimiters, good for simple data exchange.
XML and JSON: For complex data structures and additional information (metadata) about data.
# JSON preferred: 
More lightweight and easier to use compared to XML.
These file formats help organize and share data efficiently, especially when dealing with structured information.


# Chapter 3


Exploring Databases

 Different database options to choose from when an organization needs to store data.

1. Relational: A relational database is a type of database designed to store and manage data in a structured way, using tables with rows and columns.
(all these tables connect to each other)

* Each table revolves around an entity
* when sing relational database you can encrypt tabales, hide information,making sure that the right people have access to data

* sturactured way to store data, it is stored in a table that is why is called relational sinsince there table has got  a relationship

  
3. Nonrelational:  Store data differently than relational databases. They offer more flexibility in structure and data types, making them a good choice for big data, rapid development, and specific performance needs.
# types f nonerational

* Key values: this shows data as a key and a value, one key that tise to a value and another value that tise to a key(and you can retrive these value by pluggig the keys and getting the information back

* column store: stores values in seperate column and is optimised in seperate columns

* graph database: this shows different entities in the data base and how they connect to each other in a graphical way

* Document sore database: so this data base sotores data in a document or a group of documents which is called a collection you can retrive information as it relats to a single information

A document database is indeed similar to a key-value store with enforced structure.

* Key-Value Similarities: Both document and key-value databases store data using key-value pairs for retrieval.
  
* Structured Values: A key difference is in the value. Key-value databases allow for any type of data in the value field. Document databases constrain the value to a specific format, often JSON. This structure makes documents more complex than raw values but enables richer queries.

#  Structured Flexibility:

Document databases share similarities with key-value stores, using key-value pairs for data retrieval.
However, document databases add a layer of structure. The value (data) adheres to a defined format, often JSON.
# Benefits of Structure:

* This structure unlocks greater flexibility compared to key-value stores.
* While retrieving data by document key is the fastest method, document databases allow you to search within the document itself.
# Example: Social Network Profiles

* Imagine storing social media profiles
where the key is the username and the value is a JSON document containing profile details (like Figure 3.6).
* With a document database, you can efficiently search for profiles based on specific fields within the document, like finding all profiles in a particular zip code.
# Core Advantage:

* The key advantage is the ability to search and query data based on its internal structure, making it powerful for scenarios where you need to find information within documents.


# ERD line endings
![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/87d8fb2d-1107-4c37-bd4f-2a03ce702cd7)


# Unary relationship
A unary relationship is when an entity has a connection with itself.
![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/5862f6c1-ded7-40a0-8b50-1c14bb9ddfb9)

# Entity relationship diagram

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/3005294a-d0ac-4d85-b639-e0e323e4a5a2)

Relational databases translate your ERD (Entity-Relationship Diagram) into a functioning system. Here's the breakdown:

1. ERD to Reality: An ERD blueprint is transformed into a physical database design.
   
2. Tables and Columns: ERD entities become database tables, and entity attributes become columns within those tables.

3. Column Order Flexibility: The order of columns within a table doesn't inherently matter. You can specify the order when retrieving data.

4.  Data Types: Each column is assigned a data type to define the kind of information it can store (e.g., text, numbers, dates).
   
6. Schema: The Big Picture: The final product, containing all this information, is called a schema. It's essentially a detailed ERD ready for database creation.


# Database schema
![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/9f9e91aa-9601-45c6-9650-21e34b92a1e5)


# Person Data

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/12285f8d-2fe7-4ac0-8e7a-99e354b71778)





Referential integrity

Referential integrity in data analytics ensures consistency across linked tables in a relational database. It's like a set of rules that maintains data accuracy and avoids confusion. Here's the gist:

* Foreign Keys and Primary Keys: The magic relies on primary keys (unique identifiers in one table) and foreign keys (which reference those primary keys in another table).
  
* Data Consistency: Referential integrity prevents rows existing in a table if the referenced data (identified by the foreign key) doesn't exist in the linked table.

*   Imagine an "Orders" table referencing a "Customers" table by customer ID. Referential integrity stops orders from being placed for non-existent customers.
  
* Avoiding Broken Links: By enforcing these rules, referential integrity helps avoid "orphaned" data - entries with meaningless foreign key references. This keeps your data analysis accurate and reliable.
  
In short, referential integrity acts like a data guardian, ensuring relationships between tables are valid and your analysis reflects a clean, consistent picture.

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/e7219491-4130-41dd-b407-33320d279aa6)

example:

customer ERD

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/d95f8e85-f57e-4754-961f-466c7733889a)


* In this ERD you understant that A specific customer can have many addresses, while a particular address belongs to a single customer.
  
* A specific address has a single address type, while a particular address type can apply to many different addresses.
  
* A specific address has a single state, while a particular state can exist in many different addresses. 

* Implementing the relationships from an ERD enforces data constraints
*  For example, Figure the above customer ERD shows that an individual address must have a relationship with a state
* You use foreign keys to implement data constraints in a database, like the relationship connecting Address with State.

# Foreign key data constraint

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/e41d77b7-a5a5-41d6-934d-1558d600cda7)

# Relational Database Providers: 

* Oracle: A veteran player, established in 1979, offering a mature and proven database platform.
  
* Microsoft SQL Server: Developed by Microsoft as a competitor to Oracle in the relational database market..
  
* Open Source Options: MySQL, MariaDB, and PostgreSQL are open-source relational database options, known for their flexibility and community support.
  
* AWS Aurora: A cloud-based relational database service by Amazon Web Services, compatible with MySQL and PostgreSQL. It leverages the scalability of the AWS cloud platform.


The passage emphasizes that these are just a few examples, and choosing the right provider depends on your specific needs. It then lays the groundwork for a comparison between relational and non-relational databases.


# Database Use Cases

Different business needs require different database designs. While all databases store data, the database's structure needs to match its intended purpose. Business requirements impact the design of individual tables and how they are interconnected. Transactional and reporting systems need different implementation approaches to serve the people who use them efficiently. Databases tend to support two major categories of data processing: Online Transactional Processing (OLTP) and Online Analytical Processing (OLAP).


# Online Transactional Processing

A vet clinic transactional schema is the blueprint for a database that tracks activity at the clinic. It uses interconnected tables to represent different aspects, like clients, pets, appointments, and procedures.


Tables: Clients, Pets, Appointments, Procedures, Treatments (linking procedures to appointments), Medications (potentially linking to Treatments).
Relationships:
* Clients can have many Pets (one-to-many).
* Appointments are for one Pet belonging to one Client (many-to-one with both tables).
* Treatments link Appointments to Procedures (many-to-one with both).
* Medications might be linked to Treatments (many-to-many).
Customization: The schema can be extended with Staff and Invoices tables, and specific details (attributes) are chosen for each table.

This structure allows the clinic to efficiently manage pet care, appointments, billing, and potentially inventory.

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/fb19458e-4a78-45db-998e-1d01518a4832)

# Normalization
Normalization is a process for structuring a database in a way that minimizes duplication of data.

# First normal form
* (1NF) is when every row in a table is unique and every column contains a unique value.
 ![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/5a8413c2-9451-48dc-b1ca-a6ec17681303)


# Second normal form 

* (2NF) starts where 1NF leaves off. In addition to each row being unique, 2NF applies an additional rule stating that all nonprimary key values must depend on the entire primary key. 
![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/d69d4d9b-d299-42de-98a4-7f1ebe923c4d)



# Third normal form

* (3NF) builds upon 2NF by adding a rule stating all columns must depend on only the primary key. 


![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/ed3122e4-1eb1-4770-a92d-f0a99a0c464e)

# Online Analytical Processing

Online Analytical Processing (OLAP) is all about crunching data for insights, and its database design reflects that goal. Here's a summary:

* Focus on Analysis: OLAP systems prioritize data analysis for organizations.
  
* Different Structures: While OLAP and OLTP (Online Transactional Processing) can use relational databases, their structures differ fundamentally.
  
* OLTP for Transactions: OLTP databases balance read/write performance for frequent transactions, leading to a highly normalized design (3NF - Third Normal Form) to minimize data redundancy.
  
* OLAP for Big Picture: OLAP databases favor a denormalized approach. Data is spread wider within tables (compared to OLTP) to optimize complex analytical queries that read large amounts of data at once. Joining multiple normalized tables can be slow for analysis.
  
Example: Imagine analyzing a family's pet history (Figure 3.18). The normalized OLTP schema (Figure 3.14) requires joining data from five separate tables, making the query complex and slow.

In essence, OLAP prioritizes speed and ease of complex data analysis over strict data normalization practices found in OLTP systems.

# Schema Concepts

* The design of a database schema depends on the purpose it serves. Transactional systems require highly normalized databases, whereas a denormalized design is more appropriate for analytical systems.
* A data warehouse is a database that aggregates data from many transactional systems for analytical purposes. 
* Transactional data may come from systems that power the human resources, sales, marketing, and product divisions.

*  A data warehouse facilitates analytics across the entire company.
* Data marts are specialized versions of data warehouses.
* Data warehouses cater to the whole company, while data marts zoom in on specific departments.
Your example of HR data for employee trends is a great illustration.

* Raw and Unstructured: Data lakes store data "as is" without forcing it into a pre-defined structure. This includes text, images, sensor readings, and anything else you might collect.
* Flexibility: This unstructured approach allows for wider data exploration and potential future uses you might not have anticipated yet.
* Complexity: Since data isn't prepped and organized, analyzing it requires more effort from data scientists who need to understand the raw format.
* Missing Structure: Unlike relational databases where data is organized and reflects business rules, data lakes lack this built-in structure.


* Design Matters: How you structure your data warehouse or data mart (schema design) affects how efficiently you can analyze it, especially for large datasets.
* Beyond Design: Thinking about the data's life cycle is just as important as the initial design. This includes:
  * Origin: Where does the data come from (different sources)?
  * Freshness: How often does the data change (daily, monthly)?
  * Retention: How long do you need to store the data (years, archives)?


# STAr


# Data Acquisition Concepts

* Data is the fuel for data analysis, and it can come from two main sources:
 * Internal Systems: Data you collect from your own software and applications.
 * External Sources: Data purchased or obtained from outside providers.
* This section dives into how to get data from both internal and external sources for your analysis

Integration 

Data from transactional systems flow into data warehouses and data marts for analysis. Recall that OLTP and OLAP databases have different internal structures. You need to retrieve, reshape, and insert data to move data between operational and analytical environments. You can use a variety of methods to transfer data efficiently and effectively.

 # methods to transfer data efficiently and effectively.


1. Extract:  In the first phase, you extract data from the source system and place it in a staging area. The goal of the extract phase is to move data from a relational database into a flat file as quickly as possible.

2. Transform:  The second phase transforms the data. The goal is to reformat the data from its transactional structure to the data warehouse's analytical design.

3. Load:  The purpose of the load phase is to ensure data gets into the analytical system as quickly as possible.

# ELT vs ETL for data warehouses:

 ELT vs ETL for data warehouses:

* ELT (Extract, Load, Transform): Loads raw data directly into the warehouse first, then transforms it later. This can be faster for big data sets.
* ETL (Extract, Transform, Load): Transforms data outside the warehouse before loading it in. This offers more control but can be slower.
* Key Difference: ETL transforms data with external tools, while ELT uses the power of the data warehouse itself (often with SQL).
* Choosing Between Them: Consider data size, processing speed needs, technical expertise, and overall data strategy.

  # ETL Vendors

  

# Data Collection Methods
![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/611e52a5-9407-4d0c-84f7-5f7ba7bea919)


 # Working With Data

Data Manipulation: When manipulating data, one of four possible actions occurs:

* Create new data.
* Read existing data.
* Update existing data.
* Delete existing data.
  
The acronym CRUD (Create, Read, Update, Delete) is a handy way to remember these four operations.

SQL uses verbs to identify the type of activity a specific statement performs. For each CRUD activity, there is a corresponding DML verb, as Table 3.8 illustrates. These verbs are known as keywords or words that are part of the SQL language itself.


# SQL Considerations: Case Sensitivity:
SQL keywords themselves are not case-sensitive (SELECT vs. Select).
However, column names and values might be case-sensitive depending on your database setup.
Formatting:
SQL queries can span multiple lines for readability, but they will produce the same result if written on one line.
Best practices for formatting depend on your organization, but should consider clarity, efficiency, and readability.


![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/d6235136-076d-4f93-83d3-b7c035633490)

# illustrates the basic structure of a SQL query that reads from a database. SELECT, FROM, and WHERE are all reserved words that have specific meanings in SQL.

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/c92e0e48-4bab-4583-80ac-a884a0ca26ab)


The SELECT clause identifies the columns from the table(s) that are retrieved. If you want to list the name and animal type from the table , the SELECT portion of the query will look like this:

SELECT Animal_Name, Breed_Name

The FROM clause in a query identifies the source of data, which is frequently a database table. Both the SELECT and FROM clauses are required for a SQL statement to return data, as follows:

SELECT Animal_Name, Breed_Name

FROM  Animal

As the queries in Figure 3.21 illustrate, it is possible to specify more than one location for data by joining tables together.















# Chapter 4(DATA QUALITY)

WHAT THIS CHAPTER COVERS 

* Understanding the challenges of data quality. 
* Identifying common reasons for cleansing and profiling datasets.
* Executing data manipulation techniques. 
* Applying data quality control concepts.


# Data Quality Challenges
# Duplicate Data

Duplicate data occurs when data representing the same transaction is accidentally duplicated within a system. Suppose you want to open a spreadsheet on your local computer. 
The issue of duplicate data creation, primarily caused by human error.  While systems are designed to prevent duplicates, the best defense is to stop them before they happen.  The example highlights how a simple action like double-clicking can lead to unintended duplicates.  The solution? Visual warnings that alert users of the potential for duplicate entries before they confirm an action.

Duclication data resolution process 

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/f96e1973-dd41-469a-a093-b8281c03d137)

# HOW TO RESOVE DUPLICATION?

* To resolve duplicate data issues, the company has a duplicate resolution process.
*This process looks for customers with multiple billing addresses, validates the correct address, and updates the Sales database by removing the duplicate record.

# Redundant Data
Redundant data happens when the same data elements exist in multiple places within a system.
* Data redundancy is a function of integrating multiple systems.

# ILLUSTRATION OF TWO TRANSECTIONAL SYSTEEM FEEDING A SING DATA WAREHOUSE

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/776052f4-eed5-4b22-a2b1-03ca8340c5d2)

* Wayas of resolving redundant data: One approach synchronizes changes to shared data elements between the Accounting and Sales systems.
  
  illustration of resolving redundancy with an integrated ETL process

  
![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/a45cf6f4-4afa-4cb9-b3b0-ba9dfe63796b)

* Another root cause of data redundancy is an inappropriate database design
  
# Missing Values
* Missing values occur when you expect an attribute to contain data but nothing is there.
* Missing values are also known as null values.
* A null value is the absence of a value.
* A null is not a space, blank, or other character. There are situations when allowing nulls makes sense. 


# Invalid Data

* Invalid data are values outside the valid range for a given attribute.
* An invalid value violates a business rule instead of having an incorrect data type
* One has to understand the context of a system to determine whether or not a value is invalid
* Text data is more complex. One thing that leads to invalid character data is an absence of referential integrity within a database
* If two tables have a relationship but no foreign keys, the conditions for invalid character data exist
  
# Nonparametric Data

* Nonparametric data is data collected from categorical variables,
* sometimes they have a rank order associated with them

# Data Outliers

* A data outlier is a value that differs significantly from other observations in a dataset.

# Specification Mismatch

* A specification describes the target value for a component.
* A specification mismatch occurs when an individual component's characteristics are beyond the range of acceptable values.

* When data is invalid, it has values that fall outside a given range. 
* On the other hand, a specification mismatch occurs when data does not conform to its destination data type. 

* If the destination column is numeric and you have text data, you'll end up with a specification mismatch. 
* To resolve this mismatch, you must validate that the inbound data consistently maps to its target data type.


# Data Type Validation

Data type validation ensures that values in a dataset have a consistent data type. 

* Programming languages, including SQL, Python, and R, all have data type validation functions
* Use these functions to validate the data type for each column in a data file before attempting a database load.

* It is the best to detect and remediate data type issues as early as possible to ensure data is ready for analysis

# Data Manipulation Techniques
Recoding Data

* Recoding data is a technique you can use to map original values for a variable into new values to facilitate analysis.
* Recoding groups data into multiple categories, creating a categorical variable.
* A categorical variable is either nominal or ordinal.
* Nominal variables are any variable with two or more categories where there is no natural order of the categories, like hair color or eye color.
* Ordinal variables are categories with an inherent rank. For example, T-shirt size is an example of an ordinal variable, as sizes come in small, medium, large, and extra-large. Variable values fit into a fixed number of categories, similar to how lookup tables work

# Derived Variables

* derived variable is a new variable resulting from a calculation on an existing variable.
* categorical variable is an example of a derived variable.
* However, derived variables don't have to be categorical.

# Data Merge

* A data merge uses a common variable to combine multiple datasets with different structures into a single dataset. 
* Merging data improves data quality by adding new variables to your existing data.
* Additional variables make for a richer dataset, which positively impacts the quality of your analysis.
* Since a data merge adds columns to a dataset, merging gives you additional data about a specific observation.
* ETL processes commonly append data while transforming data for use in analytical environment.
  
illustration of merging disparate data

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/c9080566-97fd-4e9c-b5f3-98442fbba993)



# Data Blending

* Data blending combines multiple sources of data into a single dataset at the reporting layer.
*  While data blending is conceptually similar to the extract, transform, and load process
* Data blending differs from ETL in that it allows an analyst to combine datasets in an ad hoc manner without saving the blended dataset in a relational database.
* Data blending can reduce the burden on IT as it gives analysts the ability to merge data.
  
# Illustratuion showing the traditional ETL workflow, the analyst needs to understand the data warehouse's structure to create a visualization

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/6a5a9587-67d1-40e3-abab-7b00f807c166)

Diagram illustrating data blending

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/bba4cac7-a6b6-47c2-9e0a-42be4cb65127)

# Concatenation
Concatenation is the merging of separate variables into a single variable. 
Concatenation is a highly effective technique when dealing with a source system that stores components of a single variable in multiple columns. The need for 
* concatenation frequently occurs when dealing with date and time data. 
* Concatenation is also useful when generating address information.


A diagram showing an example of 

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/1f687d8a-c098-445d-8501-87156f24b863)

# Data Append

* A data append combines multiple data sources with the same structure, resulting in a new dataset containing all the rows from the original datasets.
* When appending data, you save the result as a new dataset for ongoing analysis.
  
![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/4df1cdf2-463a-4e25-a2ef-7c9c1d89acfa)

# Imputation
* Imputation is a technique for dealing with missing values by replacing them with substitutes. When merging multiple data sources, you may end up with a dataset with many nulls in a given column. If you are collecting sensor data, it is possible to have missing values due to collection or transmission issues.
There are many potential reasons why the data is missing. Perhaps the person is using a smart scale, and the scale lost its network connection. It could be that the person is using a manual scale and forgot to record the value. Another possibility is that the person didn't get on the scale on those four days. Regardless of the reason, the analyst has to decide how to handle the missing data.

# few approaches an analyst can use for imputing values:

* Remove Missing Data:  With this approach, you can remove rows with missing values without impacting the quality of your overall analysis.

* Replace with Zero:  With this approach, you replace missing values with a zero. Whether or not it is appropriate to replace missing data with a zero is contextual. In this case, zero isn't an appropriate value, as a person's weight should be a positive number. In addition, replacing a zero in this case has an extraordinary impact on the overall average weight.
  
* Replace with Overall Average:  Instead of using a zero, you can compute the average Weight value for all rows that have data and then replace the missing Weight values with that calculated average.

* Replace with Most Frequent (Mode):  Alternatively, you can take the most frequently occurring value, called the mode, and use that as the constant.
  
* Closest Value Average:  With this approach, you use the values from the rows before and after the missing values. For example, to replace the missing measurements for 2/13/2021 and 2/14/2021, take the values from 2/12/2021 and 2/15/2021 to compute the average.


# Reduction
When dealing with big data, it is frequently unfeasible and inefficient to manipulate the entire dataset during analysis. Reduction is the process of shrinking an extensive dataset without negatively impacting its analytical value. There are a variety of reduction techniques from which you can choose. Selecting a method depends on the type of data you have and what you are trying to analyze. Dimensionality reduction and numerosity reduction are two techniques for data reduction.

# Dimensionality Reduction

* One reduction technique is dimensionality reduction, which removes attributes from a dataset. Removing attributes reduces the dataset's overall size.
  
# Numerosity Reduction

Another technique is numerosity reduction, which reduces the overall volume of data.
One way to reduce the volume of quantitative data is by creating a histogram. You can create a histogram in Python, R, and many visualization-specific tools. A histogram is a diagram made up of rectangles, or bars, that show how frequently a specific value occurs. When creating a histogram, you can conFigure the width of a rectangle to represent a range of values.


# Aggregation

* Data aggregation is the summarization of raw data for analysis. When you are dealing with billions of individual records, a data summary can help you make sense of it all. 

# Transposition

* Transposing data is when you want to turn rows into columns or columns into rows to facilitate analysis.
* When transposing, each value from a column becomes a new column.
* To organize the data to be beneficial to leadership, you can combine transposition of the FiscalYear column with aggregation of Sales data to generate the table 


If you're curious about how the min-max normalization, consider its mathematical definition:



Let's walk through it using the Height data for AthleteID 1 in Figure 4.32. The first thing you need to do is find the minimum value of the Height column, which is 68. Then, find the maximum value of the Height column, which is 76. For AthleteID 1, the value you want to convert is 71. Once you have these values, you can plug them into the formula as follows:

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/c442ebc4-1eef-48ef-8a2c-d6d3738ddaba)


Min-max normalization is one of the most straightforward approaches to normalizing data.



# Managing Data Quality

* There are many techniques you can use to improve data quality.
* To be a successful analyst, you must recognize scenarios that create the conditions for data quality issues.
* To help you develop a mental data quality checklist, let's explore various situations and see how different data quality controls apply.


Circumstances to Check for Quality


* There are numerous circumstances where it is appropriate to implement data quality control checks. Every stop along the data life-cycle journey can impact data quality
* Errors during data acquisition, transformation, manipulation, and visualization all contribute to degrading data quality
* You should recognize the types of quality issues that can occur and have an overarching strategy to ensure the quality of your data.

Data Acquisition 

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/6a0a7cf6-c099-4a33-b20a-ce8ff8da5695)

# Automated Validation

* Data from various sources: Universities (first passage) and analytics environments (second passage) use data from multiple sources, including human-generated data.
  
* Human input and errors: Human interaction with data entry systems can introduce errors.
  
* Data validation: Automating data validation checks is crucial to prevent these errors from impacting analysis. This involves ensuring data types match and verifying the expected number of data points.
  
* Benefits: Data validation safeguards data quality, leading to more reliable results in student success analysis (first passage) or any other analytics application (second passage).



# Data Quality Dimensions


* It is essential to consider multiple attributes of data when considering its quality. Six dimensions to take into account when assessing data quality are accuracy, completeness, consistency, timeliness, uniqueness, and validity. Understanding these dimensions and how they are related will help you improve data quality.

![image](https://github.com/SesethuPotye/DATA-ANALYTICS/assets/162969678/80a0719a-04ce-4132-8173-2290b7c56bb1)

# Chapter 6 : DATA ANALYTICS TOOLS

* Spreadsheets: These are like electronic tables where you can organize data in rows and columns. They are user-friendly and great for basic data manipulation and calculations. Examples include Microsoft Excel and Google Sheets.

* Programming Languages: For more complex tasks, data analysts and scientists often use programming languages like R and Python. These languages allow for more in-depth analysis and customization of data manipulation.

* SQL (Structured Query Language): This is the language used to interact with databases. It allows users to retrieve, insert, update, and delete data stored in relational databases.

* Statistical Packages: These are specialized software programs designed for advanced statistical analysis. Examples include IBM SPSS, Stata, and Minitab. They offer a wide range of statistical tests, data visualization tools, and functionalities for tasks like hypothesis testing and regression analysis.

* Machine Learning Tools: These tools help automate the process of building models that can learn from data and make predictions. Examples include IBM SPSS Modeler and RapidMiner. They allow users to build and train various machine learning models visually, without necessarily needing to write code.

* Analytics Suites: These are comprehensive platforms that provide a complete environment for data analysis. They offer functionalities for data ingestion, cleaning, exploration, visualization, modeling, and reporting. Examples include IBM Cognos, Microsoft Power BI, and Domo.

* Cloud-Based Business Intelligence (BI) Tools: These tools leverage the cloud's scalability and flexibility for data analysis. Examples include AWS QuickSight and Tableau. They offer user-friendly interfaces for data exploration, visualization, and collaboration, often with pay-as-you-go pricing models.

* Data Visualization Tools: These tools specialize in creating visually appealing and informative charts, graphs, and other visual representations of data. Tableau and Qlik are popular examples. They allow users to communicate data insights effectively to a wider audience.

Choosing the right data analysis tool depends on the specific needs of the project, the expertise of the users, and the size and complexity of the data. This overview provides a starting point for understanding the various options available.



# Chapter 7: Data Visualization with Reports and Dashboards

This chapter outlines key concepts for creating effective reports and dashboards using data visualization techniques.

# Understanding the Business Requirements

* Who is the audience? Knowing your target audience is crucial for tailoring the report or dashboard to their needs.

* Data access: Ensure you have access to the appropriate data sources for your project.

* Report distribution:
Pull approach: Users access the report from a central location (e.g., webpage) at their convenience.
  * Push approach: Reports are automatically delivered to users 
    upon completion.
 * Blended approach: A combination of both, where users are notified of report updates but access it centrally.
   
# Understanding Report Design Elements

# The 5Cs of visualizations: A framework for creating clear and efficient reports and dashboards:

* Control: Focuses audience attention on key information.
* Correctness: Ensures data accuracy and avoids errors.
* Clarity: Employs appropriate visuals and easy-to-read elements.
* Consistency: Maintains a consistent visual style throughout.
* Concentration: Minimizes clutter and unnecessary details.
  
# Report Cover Page

* Concise title: Briefly describes the report's content.
* Significant insight: Ideally, conveys a key finding from the report.
  
  # Design Elements

* Color schemes: Choose a color palette that complements your * message (monochromatic or complementary).
* Page layout: Arrange components for user-friendliness.
* Fonts: Select fonts that are easy to read (serif or sans serif).
* Charts: Include clear and well-labeled charts.
  
  # Documentation Elements

* Version numbers: Track changes made to the report over time.
* Reference data sources: Indicate the data's source and update frequency.
* Frequently Asked Questions (FAQs): Address common user questions.
* Appendix: Include supplementary details not suitable for the main body.
  
* Understanding Dashboard Development Methods






































































































































































